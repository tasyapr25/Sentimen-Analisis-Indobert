# -*- coding: utf-8 -*-
"""3PreprocessingData1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fFRCEh6RWEi2vK0BlQ_vdaujYs2rbhlb
"""

import pandas as pd
import re
import seaborn as sns
import matplotlib.pyplot as plt
import ast

!pip install Sastrawi
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory

import pandas as pd
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

df = pd.read_csv ("/content/drive/MyDrive/sentimen/dataset/tweet_relevan_done.csv", encoding="latin1")
df.head()

df = df[['full_text']]
df

"""# Langkah 1. Case Folding"""

# 1. Case folding: ubah teks menjadi huruf kecil semua
df['case_folded'] = df['full_text'].str.lower()

# 2. Tampilkan contoh hasilnya (sebelum dan sesudah)
df[['full_text', 'case_folded']].head(10)

"""# Langkah 2. Cleaning"""

# 2. Cleaning: hapus URL, mention, hashtag, angka, tanda baca, dsb
def clean_text(text):
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)  # URL
    text = re.sub(r'\@\w+|\#','', text)  # mention & hashtag
    text = re.sub(r'[0-9]+', '', text)  # angka
    text = re.sub(r'[^\w\s]', '', text)  # tanda baca
    text = re.sub(r'\n',' ', text)  # newline
    text = re.sub(r'\s+',' ', text).strip()  # spasi ganda
    return text

# Terapkan ke kolom baru
df['clean_text'] = df['case_folded'].apply(clean_text)

df[['case_folded', 'clean_text']].head()

"""# Langkah 3. Tokenizing"""

# Tokenisasi dengan split
df['tokens'] = df['clean_text'].apply(lambda x: x.split())

# Tampilkan hasil
df[['clean_text', 'tokens']].head()

"""# Langkah 4. Stopwords Removal"""

!pip install Sastrawi

from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory

# Ambil daftar stopwords Bahasa Indonesia
factory = StopWordRemoverFactory()
stopwords = set(factory.get_stop_words())

sentiment_stopwords = [
    'nya', 'nih', 'sih', 'dong', 'loh', 'deh', 'aja', 'lagi', 'kayak', 'gitu', 'banget', 'pun', 'lah',
    'ini', 'itu', 'wkwk', 'hmm', 'eh', 'ya', 'kalo', 'kalau', 'karena', 'jadi', 'sudah',
    'di', 'yang', 'yg', 'dan', 'sama', 'dari', 'untuk', 'dengan', 'sudah',
    'banyak', 'ke', 'apa', 'juga', 'masih', 'mau', 'adalah', 'kan',
    'pak', 'bu', 'lo', 'gua', 'gue', 'aku', 'kamu', 'anda', 'saya', 'orang', 'semua', 'kita', 'mereka', 'dia'
]


df['tokens_clean'] = df['tokens'].apply(lambda x: [word for word in x if word not in sentiment_stopwords])

# Lihat hasilnya
df[['tokens', 'tokens_clean']].head()

"""# Langkah 5. Slang Word Normalization"""

# Pastikan kamus tersedia
slang_df = pd.read_csv('/content/drive/MyDrive/sentimen/dataset/colloquial-indonesian-lexicon.csv', usecols=['slang', 'formal'])
slang_dict = dict(zip(slang_df['slang'], slang_df['formal']))

# Fungsi normalisasi slang
def normalize_token_list(token_list):
    return [slang_dict.get(token, token) for token in token_list]

df['normalized_tokens'] = df['tokens_clean'].apply(normalize_token_list)

# Lihat hasil
df[['tokens_clean', 'normalized_tokens']].head()

"""# langkah 6. Stemming"""

# Import dulu kalau belum
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

# Inisialisasi stemmer
factory = StemmerFactory()
stemmer = factory.create_stemmer()

# Fungsi stemming per token dalam list
def stem_token_list(token_list):
    return [stemmer.stem(token) for token in token_list]

# Terapkan stemming ke kolom normalized_tokens
df['stemmed_list'] = df['normalized_tokens'].apply(stem_token_list)

# Gabungkan kembali
df['stemmed_text'] = df['stemmed_list'].apply(lambda x: ' '.join(x))

# Lihat hasil
df[['normalized_tokens', 'stemmed_list', 'stemmed_text']].head()

# Path output
output_path = '/content/drive/MyDrive/sentimen/dataset/preprocessed_tweets.csv'

# Pilih kolom-kolom yang ingin disimpan (sesuaikan dengan apa saja yang sudah kamu buat)
cols = [
    'full_text',
    'case_folded',
    'clean_text',
    'tokens',
    'tokens_clean',
    'normalized_tokens',   # atau 'normalized_list'
    'stemmed_list',
    'stemmed_text'
]

# Simpan ke CSV
df[cols].to_csv(output_path, index=False)

print(f"âœ… Semua data pre-processing berhasil disimpan di:\n{output_path}")

from google.colab import drive
drive.mount('/content/drive')

from itertools import chain
print("Jumlah total kata setelah dibersihkan:", len(list(chain.from_iterable(df['stemmed_list']))))

"""## VISUALISASI DATA"""

import pandas as pd
import ast
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from collections import Counter
import seaborn as sns

# Load data dan proses ulang
df = pd.read_csv('/content/drive/MyDrive/sentimen/dataset/preprocessed_tweets.csv')
df['stemmed_list'] = df['stemmed_list'].apply(ast.literal_eval)
df['stemmed_text'] = df['stemmed_list'].apply(lambda x: ' '.join(x))

# Visualisasi Word Cloud
all_words = ' '.join(df['stemmed_text'])
wordcloud = WordCloud(width=900, height=240, background_color='white').generate(all_words)

plt.figure(figsize=(12, 6))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title("Word Cloud dari Semua Teks yang Sudah Distem", fontsize=16)
plt.show()

# Visualisasi Barplot 20 kata paling umum ---
words = all_words.split()
counter = Counter(words)
common_words = counter.most_common(20)

words, counts = zip(*common_words)
plt.figure(figsize=(12,6))
sns.barplot(x=list(words), y=list(counts))
plt.xticks(rotation=45)
plt.title("20 Kata Paling Sering Muncul (Setelah Preprocessing)")
plt.ylabel("Frekuensi")
plt.xlabel("Kata")
plt.show()

# Distribusi panjang teks
df['text_length'] = df['stemmed_text'].apply(lambda x: len(x.split()))
plt.figure(figsize=(10,5))
sns.histplot(df['text_length'], bins=20, kde=True)
plt.title("Distribusi Panjang Teks (jumlah kata)")
plt.xlabel("Jumlah kata")
plt.ylabel("Jumlah dokumen")
plt.show()